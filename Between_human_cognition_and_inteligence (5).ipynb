{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWNmTSxVzOQJ"
      },
      "source": [
        "[This GitHub repository](https://github.com/shirashko/bias-in-llms/tree/main) contains all relevant info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLE6PhhGCUQ0"
      },
      "source": [
        "# Between Artificial and Human Intelligence\n",
        "\n",
        "## Anchoring Effect\n",
        "### Examining Anchoring Bias in Language Models\n",
        "\n",
        "#### Overview\n",
        "This project investigates whether large language models (LLMs) exhibit the cognitive bias known as the anchoring effect. Originally identified by Professors Amos Tversky and Daniel Kahneman, this bias influences human judgment such that initial information, even if incorrect or irrelevant, can significantly affect decision-making. This effect persists even in cases where the individual's awareness of the anchor's irrelevance. For instance, if someone claims that \"the average price of an iPhone is $1,\" we intuitively know this to be extremely improbable based on our general knowledge of the market. However, this statement might still subconsciously influence us to lower our estimate of what an iPhone should cost, compared to our assessment in the absence of such an anchor.\n",
        "\n",
        "#### The Anchoring Effect\n",
        "Anchoring bias occurs when an initial piece of information provided at the time of decision-making influences perceptions. For example, if one group is informed that Kennedy was 38 at the time of his assassination, and another group is told he was 55, the group exposed to the higher age will likely estimate his age at death to be greater. Tversky posited that the anchor establishes a mental baseline, which is then insufficiently adjusted. Kahneman suggested that the anchor primes associated thoughts, which then influence subsequent judgments.\n",
        "\n",
        "#### Relevance to Language Models\n",
        "This study aims to assess if LLMs, similar to humans, can be influenced by anchors in situations such as estimating product prices. Given that LLMs process inputs based on both the provided data and their extensive pre-training on mainly human-generated text, they might also reflect human biases. This expectation is based on the understanding that their training data typically embodies these biases.\n",
        "\n",
        "---\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <a href=\"https://www.google.com\">\n",
        "        <img src=\"https://images.prismic.io/thedecisionlab/ac7d00b4-1c09-4c47-97f3-72d7bf8c922d_anchoring-effect-bias.jpeg?auto=compress,format&rect=0,1,2387,1667&w=2388&h=1668\" alt=\"Anchoring Effect\" title=\"Anchoring Effect\" width=\"400\"/>\n",
        "      </a>\n",
        "    </td>\n",
        "    <td>\n",
        "      <img src=\"https://cognitivebias.io/uploads/ybias/image-6492ed4f5284e.png\" alt=\"Cognitive Bias\" title=\"Cognitive Bias\" width=\"400\"/>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vOZORWy6ky6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade\n",
        "!pip install -q -U accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txBrQO6koIMe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import re\n",
        "from scipy.stats import ttest_rel\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import bitsandbytes as bnb\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3azV1a-585ff"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "# Sample and Price Settings\n",
        "NUMBER_OF_EXAMPLES = 120  # Total number of examples to be generated or processed\n",
        "PRICE_RANGE = (0, 1000)  # The range of prices to be used in simulations or examples\n",
        "\n",
        "# System Configuration\n",
        "SEED = 42  # Seed value for any random number generation to ensure reproducibility\n",
        "RESPONSE_EXTENSION_LENGTH = 10  # Number of tokens to extend beyond the input in generated responses\n",
        "\n",
        "# File Paths\n",
        "DATA_FILE_PATH = '/content/data.xlsx'  # Path to the primary data file\n",
        "ENHANCED_DATA_FILE_PATH = '/content/few_shot_data.xlsx'  # Path to the data file with enhanced prompts\n",
        "RESULT_FILE_PATH = '/content/results.xlsx'  # Path where the primary results will be saved\n",
        "BENCHMARK_BIAS_ANALYSIS_FILE_PATH = '/content/analysis_results.xlsx'  # Path for saving benchmark analysis results\n",
        "DEBIASING_ANALYSIS_FILE_PATH = '/content/debiasing_analysis_results.xlsx'  # Path for saving debiasing analysis results\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = 'facebook/opt-350m'  # Default model for running analyses\n",
        "# List of models to be used in the study, representing diverse families and capabilities\n",
        "MODELS_NAMES = ['facebook/opt-350m', 'EleutherAI/gpt-j-6B', 'mrm8488/mistral-7b-ft-h4-no_robots_instructions']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXT4eadi4lac"
      },
      "source": [
        "To ensure easy access and interaction with Google Drive for backup purposes, use the following command to authorize the session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuoFxV4vCDYu"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgIzCefH04qo"
      },
      "source": [
        "## Benchmark Development\n",
        "The benchmark comprises over 100 examples designed to investigate how anchoring prices can influence the judgments of large language models (LLMs). These examples cover a variety of contexts where initial price information might act as an anchor, potentially affecting the model outputs.\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=13rJQ7ANk60BzbdsejlfewKY3u7ZQ_G-O\" alt=\"Illustrative Chart for Benchmark\" style=\"width: 100%; max-width: 600px;\">\n",
        "\n",
        "\n",
        "### Structure of the Benchmark:\n",
        "\n",
        "#### Benchmark Structure:\n",
        "- Product: The benchmark uses a predefined list of products. Each product appears 2 to 3 times with different anchor prices.\n",
        "- Anchor Price: Each product is associated with a randomly generated price within a range of $ [0, 1000].\n",
        "\n",
        "#### Prompt Structure Overview\n",
        "Each scenario within the benchmark utilizes a carefully structured approach to prompts, specifically designed to elicit product price estimations and assess the impact of anchoring bias. Below is a detailed description of the prompt templates employed:\n",
        "\n",
        "- **Basic Prompt**: This prompt sets a neutral ground for price estimation, devoid of any anchoring bias.\n",
        "  ```python\n",
        "  basic_prompt = f\"A fair market price for a {product} is typically \"\n",
        "  ```\n",
        "\n",
        "- **Anchored Prompt**: Begins with an anchor price to examine its effect on the model's estimation and then continues with the basic prompt to complete the price assessment in a uniform manner.\n",
        "  ```python\n",
        "  anchor_prefix = f\"In some sale, the price for a high-quality {product} was {anchor_price} dollars. \"\n",
        "  anchored_prompt = anchor_prefix + basic_prompt\n",
        "  ```\n",
        "  The anchored prompt delicately balances the introduction of a specific price anchor with the uniformity of the query, allowing for a nuanced analysis of the anchor's influence.\n",
        "  \n",
        "\n",
        "\n",
        "#### Application in Benchmark\n",
        "\n",
        "- **Diverse Scenarios**: Utilizes prompt variations across a wide array of products and anchor prices to explore the anchoring effect in different contexts. Randomly assigned anchor prices simulate the variability consumers might encounter in real-life scenarios.Each product scenario features multiple variations with different, randomly chosen anchor prices. This randomness is intended to simulate diverse pricing information that consumers encounter and to test the model's responses even when the anchor is clearly irrelevant or unlikely to be reliable. This approach aims to evaluate the model's resilience to such cognitive biases under varied conditions of awareness, checking whether anchoring effects persist despite general knowledge suggesting the irrelevance of the anchor.\n",
        "\n",
        "- **Controlled Comparisons**: Comparisons between scenarios with anchors and those without help quantify the influence of anchoring on decision-making processes. Using random anchor prices enriches the dataset, providing a clearer picture of how anchoring affects model behavior across a spectrum of initial cues.\n",
        "\n",
        "This benchmark serves as a strategic tool for systematically examining the susceptibility of LLMs to anchor bias. It highlights the need for developing technologies that can recognize and mitigate biases analogous to those in human judgment. All prompts should conclude with a basic prompt, inviting the model to complete it with a price estimation.\n",
        "\n",
        "For a comprehensive exploration and to access the complete dataset, visit the [benchmark repository](https://github.com/shirashko/bias-in-llms/blob/main/input/data.xlsx).\n",
        "\n",
        "This study not only identifies the presence of anchoring bias but also explores debiasing techniques to enhance the accuracy and impartiality of AI judgments.\n",
        "\n",
        "Below is the script used to create the benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9ECK67myS0H"
      },
      "outputs": [],
      "source": [
        "def create_benchmark_data_with_prompts(product, price):\n",
        "    \"\"\"\n",
        "    Generate statements with price context and basic/anchored prompts for each product.\n",
        "    \"\"\"\n",
        "    basic_prompt = f\"A fair market price for a {product} is typically \"\n",
        "    anchored_prompt = f\"In some sale, the price for a high-quality {product} was {price} dollars. A fair market price for a {product} is typically \"\n",
        "    return product, price, basic_prompt, anchored_prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of products to use\n",
        "products = [\n",
        "    'smartphone', 'laptop', 'headphones', 'bicycle', 'watch',\n",
        "    'backpack', 'espresso machine', 'running shoes', 'gaming console',\n",
        "    'car', 'blender', 'desk chair', 'sunglasses', 'bookshelf',\n",
        "    'kitchen mixer', 'digital camera', 'fitness tracker', 'tent',\n",
        "    'suitcase', 'grill', 'snowboard', 'hiking boots', 'drone',\n",
        "    'graphic tablet', 'electric kettle', 'wireless charger', 'yoga mat',\n",
        "    'board game', 'action camera', 'water bottle', 'office desk', 'guitar',\n",
        "    'puzzle', 'wine rack', 'coffee table', 'novel', 'skateboard',\n",
        "    'sleeping bag', 'lamp', 'flower pot', 'photo frame', 'wall art',\n",
        "    'tablecloth', 'candle holder', 'throw pillow', 'garden tools',\n",
        "    'bath towel', 'cutlery set', 'wine glasses', 'yarn', 'paint set',\n",
        "    'notebook', 'pen', 'desk organizer', 'alarm clock', 'calculator'\n",
        "]\n",
        "\n",
        "# Generate initial data\n",
        "data = [create_benchmark_data_with_prompts(product, random.randint(*PRICE_RANGE)) for product in products for _ in range(3)]\n",
        "data = data[:NUMBER_OF_EXAMPLES]\n",
        "\n",
        "# Create a DataFrame with initial columns\n",
        "column_names = ['Product', 'Anchor Price', 'Basic Prompt', 'Anchored Prompt']\n",
        "df_initial = pd.DataFrame(data, columns=column_names)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "df_initial.to_excel(DATA_FILE_PATH, index=False)\n",
        "\n",
        "print(f\"Initial Excel file created at {DATA_FILE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlTh_Sm7or0p",
        "outputId": "f1050737-44a2-490a-cc2f-d9fb4d6e3ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Excel file created at /content/data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8xZhi7D3mg1"
      },
      "source": [
        "### Utility Functions\n",
        "This section details the utility functions that were used in executing the benchmark. These functions are designed to streamline various aspects of setting up and running the scenarios, ensuring the process is efficient and consistent. Below, I provide a comprehensive explanation of each function, including its purpose and how it integrates into the overall benchmark execution workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P0ACK4NvZVC"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Sets the seed for generating random numbers to ensure reproducibility.\n",
        "    This sets the seed for both the CPU and GPU (if available).\n",
        "\n",
        "    Parameters:\n",
        "    - seed (int): The seed value for random number generators.\n",
        "\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)  # Set the seed for CPU\n",
        "    if torch.cuda.is_available():  # Check if GPU is available\n",
        "        torch.cuda.manual_seed_all(seed)  # Set the seed for all GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Y4p5lnfSky"
      },
      "outputs": [],
      "source": [
        "def initialize_model(model_name):\n",
        "    \"\"\"\n",
        "    Initializes and returns a quantized transformer model, its tokenizer, and an accelerator object.\n",
        "    This function is specifically tailored for causal language models and includes quantization\n",
        "    settings using the bitsandbytes library.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): The name of the pretrained model.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Contains the model, tokenizer, and accelerator objects.\n",
        "    \"\"\"\n",
        "    set_seed(SEED)  # Ensure reproducibility\n",
        "\n",
        "    # Load the tokenizer for the specified model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Configure the model for quantization\n",
        "    quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)\n",
        "\n",
        "    # Load the model with the specified quantization config and map it to the appropriate device\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map=\"auto\")\n",
        "\n",
        "    # Initialize the accelerator for distributed or optimized single device execution\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    # Prepare the model with the accelerator and set it to evaluation mode\n",
        "    model = accelerator.prepare(model)\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer, accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj31x5S87UJz"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, accelerator, prompt, base_length=RESPONSE_EXTENSION_LENGTH):\n",
        "    \"\"\"\n",
        "    Generate text based on the provided prompt using the model.\n",
        "\n",
        "    Parameters:\n",
        "        model: The language model for text generation.\n",
        "        tokenizer: Tokenizer for encoding and decoding the prompt.\n",
        "        accelerator: Accelerator object for device placement.\n",
        "        prompt: The input text prompt for the model.\n",
        "        base_length: The additional maximum length for generated text.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text, excluding the portion of the prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode the prompt into tensor of token IDs.\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Move the tensor to the appropriate device (CPU or GPU).\n",
        "    input_ids = input_ids.to(accelerator.device)\n",
        "\n",
        "    # Calculate the total length the generated text should be, including the given prompt\n",
        "    total_length = input_ids.shape[1] + base_length\n",
        "\n",
        "    # Decode the input prompt once for later use in slicing the generated text\n",
        "    decoded_prompt = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Generate text using the model without updating model weights (inference mode).\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=total_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2  # Helps prevent the model from repeating the same two tokens in a loop, increasing text diversity and quality.\n",
        "        )\n",
        "\n",
        "    # Decode the generated token IDs to text.\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Return the generated text, excluding the initially given prompt part to ensure only new, generated content is returned.\n",
        "    return generated_text[len(decoded_prompt):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5st4eyo_BcnW"
      },
      "outputs": [],
      "source": [
        "def save_results(result_df, result_file_path):\n",
        "    \"\"\"\n",
        "    Saves the results to an Excel file.\n",
        "\n",
        "    Parameters:\n",
        "    - result_df (DataFrame): DataFrame containing all the results.\n",
        "    - result_file_path (str): Path to save the Excel file.\n",
        "    \"\"\"\n",
        "    result_df.to_excel(result_file_path, index=False)\n",
        "    print(f\"Results have been saved to {result_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_file_path):\n",
        "    \"\"\"\n",
        "    Loads data from an Excel file into a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - data_file_path (str): Path to the Excel file to be loaded.\n",
        "\n",
        "    Returns:\n",
        "    - df (DataFrame): DataFrame containing the loaded data.\n",
        "\n",
        "    Raises:\n",
        "    - FileNotFoundError: If the Excel file cannot be found at the specified path.\n",
        "    - Exception: For general exceptions that might occur during the file reading process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(data_file_path)\n",
        "        print(f\"Data has been successfully loaded from {data_file_path}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {data_file_path} does not exist.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the data: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "kt241l6-c7PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmWmkPJ24qWG"
      },
      "source": [
        "## Post Processing\n",
        "Processing the responses from the LLMs is essential to extract optimal price estimations effectively. The functions listed below have proven to be particularly effective in processing the responses generated by the models I chose to use in the study.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJo6yZ-27eUj"
      },
      "outputs": [],
      "source": [
        "# Post processing for facebook/opt-350m model\n",
        "\n",
        "def extract_and_validate_price(response):\n",
        "    \"\"\"Extract price from the response and validate if it falls within the predefined range.\n",
        "      This function returns the first number it finds in the text in the predefined valid range.\n",
        "\n",
        "    Parameters:\n",
        "        response (str): The text response from the model that might contain price.\n",
        "\n",
        "    Returns:\n",
        "        int or None: The extracted price if valid, or None if no valid price is found.\n",
        "    \"\"\"\n",
        "    # Extract prices using regex. The pattern covers numbers with commas\n",
        "    prices = re.findall(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b', response)\n",
        "    # Convert extracted price strings to float, remove commas, and validate against the range\n",
        "    valid_prices = [float(price.replace(',', '')) for price in prices if PRICE_RANGE[0] <= float(price.replace(',', '')) <= PRICE_RANGE[1]]\n",
        "    if valid_prices:\n",
        "        # Return the first valid price found\n",
        "        return valid_prices[0]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9oWKgLM-_2f"
      },
      "outputs": [],
      "source": [
        "# Post processing for 'EleutherAI/gpt-j-6B', 'mrm8488/mistral-7b-ft-h4-no_robots_instructions' models\n",
        "\n",
        "def extract_and_average_validate_price(response):\n",
        "    \"\"\"Extract and validate the average price from a response containing price ranges.\n",
        "\n",
        "    Parameters:\n",
        "        response (str): The text response that might contain price ranges.\n",
        "\n",
        "    Returns:\n",
        "        float or None: The average of the extracted price range if it falls within the acceptable range, otherwise None.\n",
        "    \"\"\"\n",
        "    # Extract prices using a pattern that captures typical expressions of ranges\n",
        "    prices = re.findall(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b', response.replace(',', ''))\n",
        "    numbers = [float(price) for price in prices]\n",
        "    if numbers:\n",
        "        average_price = sum(numbers) / len(numbers)\n",
        "        # Validate if the average price falls within the predefined range\n",
        "        if PRICE_RANGE[0] <= average_price <= PRICE_RANGE[1]:\n",
        "            return average_price\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark Development - Assessing Bias Presence\n",
        "\n",
        "This section will detail the methodologies and criteria used to evaluate the presence of bias within the benchmark scenarios.\n",
        "\n",
        "The benchmark evaluates anchoring bias in price estimations by conducting paired t-tests on the absolute differences from the anchor price between the basic and anchored prompts.\n",
        "\n",
        "#### Key Steps:\n",
        "1. **Data Filtering**: Rows missing any necessary price or anchor information are excluded to ensure the analysis uses only complete data sets.\n",
        "2. **Difference Calculation**: Absolute differences from the anchor price are calculated for each type of prompt. This quantifies how closely each prompt's price estimation adheres to the anchor.\n",
        "3. **Statistical Testing**:\n",
        "   - **Basic vs. Anchored**: Compares how basic (control) and anchored prompts differ in their proximity to the anchor price, testing the direct influence of the anchor.\n",
        "\n",
        "#### Appropriateness of the Tests:\n",
        "- **Paired T-Test** fit here due to the related nature of the data sets—each product is assessed under all conditions, allowing to directly compare their responses within the same experimental framework. This test helps determine if significant statistical differences exist between the groups, indicating the presence of anchoring bias.\n",
        "\n",
        "The results, including T-Statistics and P-Values, are displayed in a DataFrame and saved to an Excel file, facilitating a clear and accessible presentation of findings. This rigorous approach ensures that the conclusions about anchoring bias and the potential for debiasing are grounded in statistically valid comparisons.\n",
        "\n",
        "### Understanding the Statistical Metrics Results: T-Statistic and P-Value\n",
        "\n",
        "For an enhanced explanation of the paired t-test, you can visit [this website](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/paired-sample-t-test/#:~:text=Paired%20T%2DTest-,Paired%20T%2DTest,resulting%20in%20pairs%20of%20observations.), which provides a detailed overview of the methodology.\n",
        "\n",
        "#### T-Statistic:\n",
        "- **Definition**: The T-Statistic is a measure of the size of the difference relative to the variation in the sample data. In simpler terms, it shows how significant the differences between the groups are. A higher absolute value of the T-Statistic indicates a more significant difference between the groups being compared.\n",
        "- **Expectations**:\n",
        "  - A positive T-Statistic in the context of the benchmark (Basic vs. Anchored) suggests that the second group (Anchored) has price estimates that are closer to the anchor price compared to the first group (Basic).\n",
        "  - A negative T-Statistic would suggest that the first group's estimates are closer to the anchor price than the second group's.\n",
        "- **Interpretation**: In benchmark scenarios:\n",
        "  - For **Basic vs. Anchored**, a positive T-Statistic indicates an anchoring effect where the presence of an anchor price influences the model to estimate closer to that anchor.\n",
        "\n",
        "#### P-Value:\n",
        "- **Definition**: The P-Value measures the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. In this context, the null hypothesis typically states that there is no difference in mean distances from the anchor between the two groups.\n",
        "- **Expectations**:\n",
        "  - A P-Value less than 0.05 (typically used as a threshold for statistical significance) suggests that the differences observed are statistically significant and not likely due to chance.\n",
        "  - A P-Value greater than 0.05 indicates that the differences are not statistically significant, suggesting that the variation could be due to random chance rather than the effect of the anchor (or debiasing strategies).\n",
        "- **Interpretation**:\n",
        "  - A low P-Value in the **Basic vs. Anchored** comparison reinforces the presence of an anchoring bias.\n",
        "\n",
        "#### Mean Differences:\n",
        "- **Explanation**: The mean differences calculated from the anchor price provide a direct measure of how far, on average, the price estimates deviate from the anchor price under each prompt condition.\n",
        "- **Expectations**:\n",
        "  - Lower absolute mean differences for the Anchored condition compared to the Basic condition suggest a stronger anchoring effect.\n",
        "\n",
        "The combination of T-Statistics and P-Values offers a robust framework for evaluating the effectiveness of the prompts in managing anchoring bias. By examining these metrics, we gain insights into how different prompt types influence model behavior in pricing tasks, providing a measure to test anchor bias in LLMs."
      ],
      "metadata": {
        "id": "e5plhJu-gHWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_analysis(result_df, analysis_file_path, comparison_columns):\n",
        "    \"\"\"\n",
        "    Performs a paired t-test and calculates mean differences for a specified comparison of price estimations\n",
        "    relative to the anchor price, to assess the anchoring effect for a specific prompt comparison.\n",
        "\n",
        "    Parameters:\n",
        "        result_df (pd.DataFrame): DataFrame with columns for price estimations, anchor prices.\n",
        "        analysis_file_path (str): Path where the Excel report of the analysis results will be saved.\n",
        "        comparison_columns (tuple): Pair of columns to compare, e.g., ('Basic Estimated Price', 'Anchored Estimated Price').\n",
        "\n",
        "    Outputs:\n",
        "        Excel file: Saves the statistical analysis results to an Excel file.\n",
        "        Console output: Prints the t-test results and confirms that data has been saved.\n",
        "    \"\"\"\n",
        "    # Extract comparison column names\n",
        "    col_a, col_b = comparison_columns\n",
        "\n",
        "    # Drop rows where any necessary price information or anchor information is missing\n",
        "    filtered_df = result_df.dropna(subset=[col_a, col_b, 'Anchor Price'])\n",
        "    num_examples = len(filtered_df)\n",
        "    print(f\"The analysis is using {num_examples} examples\")\n",
        "    if (num_examples < 50):\n",
        "      print(\"Not enough data points for statistical tests.\")\n",
        "      return\n",
        "\n",
        "    # Calculate differences from the anchor for each column\n",
        "    diffs_a = abs(filtered_df[col_a] - filtered_df['Anchor Price'])\n",
        "    diffs_b = abs(filtered_df[col_b] - filtered_df['Anchor Price'])\n",
        "\n",
        "    # Calculate the mean of the differences\n",
        "    mean_diff_a = diffs_a.mean()\n",
        "    mean_diff_b = diffs_b.mean()\n",
        "\n",
        "    # Perform t-test on the differences\n",
        "    t_stat, p_value = ttest_rel(diffs_a, diffs_b)\n",
        "    t_test_results = pd.DataFrame({\n",
        "        'Comparison': [f'{col_a} vs. {col_b}'],\n",
        "        'T-Statistic': [t_stat],\n",
        "        'P-Value': [p_value],\n",
        "        'Mean Absolute Difference for ' + col_a: [mean_diff_a],\n",
        "        'Mean Absolute Difference for ' + col_b: [mean_diff_b]\n",
        "    })\n",
        "\n",
        "    # Save results to Excel\n",
        "    t_test_results.to_excel(analysis_file_path, index=False)\n",
        "\n",
        "    print(\"Analysis Results for the specified comparison have been saved:\")\n",
        "    print(t_test_results)"
      ],
      "metadata": {
        "id": "a01MJmLDgPkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlC23gT6_e-7"
      },
      "source": [
        "## Executing the Benchmark\n",
        "\n",
        "This code segment illustrates the comprehensive workflow of the study, beginning with the execution of the benchmark on a single dataset example using the selected model. It also encompasses the debiasing component of the study, demonstrating the application of the strategies utilized to mitigate bias.\n",
        "\n",
        "### Overview of `execute_benchmark` Function\n",
        "\n",
        "This function is designed to process a dataset by generating and analyzing text responses from a pre-trained model. It handles different types of prompts— Basic and Anchored, to evaluate how each influences the model's output, focusing on price estimations.\n",
        "\n",
        "#### Key Steps:\n",
        "\n",
        "1. **Generate Responses:** For each data entry, the model generates responses based on Basic and Anchored prompts.\n",
        "2. **Extract Prices:** Prices are extracted and validated from each model response, ensuring they reflect valid and reasonable estimations.\n",
        "3. **Save Results:** All relevant data, including product details, model responses, and price estimates are saved to an Excel file for further analysis.\n",
        "\n",
        "Lastly, the response analysis is conducted and the results are saved to an Excel file as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mYcz4RGf2Kl"
      },
      "outputs": [],
      "source": [
        "basic_prompt = \"A fair market price for a smartphone product is typically\"\n",
        "anchored_prompt = \"In some sale, the price for a high-quality smartphone was 386 dollars. A fair market price for a smartphone product is typically \"\n",
        "guided_prompt = \"In some sale, the price for a high-quality smartphone was 386 dollars. Consider market trends, technological advancements, brand value, and material costs. A fair market price for a smartphone product is typically \"\n",
        "\n",
        "basic_response = generate_text(model, tokenizer, accelerator, basic_prompt)\n",
        "anchored_response = generate_text(model, tokenizer, accelerator, anchored_prompt)\n",
        "guided_response = generate_text(model, tokenizer, accelerator, guided_prompt)\n",
        "\n",
        "processed_basic_response = extract_and_validate_price(basic_response)\n",
        "processed_anchored_response = extract_and_validate_price(basic_response)\n",
        "processed_guided_response = extract_and_validate_price(guided_response)\n",
        "\n",
        "print(\"Response for prompt:\", basic_response)\n",
        "print(\"Processed Response for prompt:\", processed_basic_response)\n",
        "print(\"Response without Guidance:\", anchored_response)\n",
        "print(\"Processed Response without Guidance:\", processed_anchored_response)\n",
        "print(\"Response with Guidance:\", guided_response)\n",
        "print(\"Processed Response with Guidance:\", processed_guided_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oLwFLvEmCmg"
      },
      "outputs": [],
      "source": [
        "def execute_benchmark(df, model, tokenizer, accelerator):\n",
        "    \"\"\"\n",
        "    Executes benchmark scenarios for a model and processes responses to extract price estimations for bias check.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): DataFrame containing the prompts and other necessary information.\n",
        "    - model: The language model for text generation.\n",
        "    - tokenizer: Tokenizer for encoding and decoding the prompt.\n",
        "    - accelerator: Accelerator object for device placement.\n",
        "\n",
        "    Returns:\n",
        "    - result_df (DataFrame): DataFrame with results from the basic and anchored prompts.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for index, row in df.iterrows():\n",
        "        basic_response = generate_text(model, tokenizer, accelerator, row['Basic Prompt'])\n",
        "        anchored_response = generate_text(model, tokenizer, accelerator, row['Anchored Prompt'])\n",
        "\n",
        "        basic_price = extract_and_validate_price(basic_response)\n",
        "        anchored_price = extract_and_validate_price(anchored_response)\n",
        "\n",
        "        results.append({\n",
        "            'Anchor Price': row['Anchor Price'],\n",
        "            'Product': row['Product'],\n",
        "            'Basic Response': basic_response,\n",
        "            'Basic Estimated Price': basic_price,\n",
        "            'Anchored Response': anchored_response,\n",
        "            'Anchored Estimated Price': anchored_price\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpgdmtG93EvY"
      },
      "outputs": [],
      "source": [
        "# Load data from Excel file\n",
        "load_data(DATA_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98wg9uqxCJrQ"
      },
      "outputs": [],
      "source": [
        "# Initialize model and tokenizer\n",
        "model, tokenizer, accelerator = initialize_model(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFMRF3OgCHXC"
      },
      "outputs": [],
      "source": [
        "# Execute bias check\n",
        "result_df = execute_benchmark(df, model, tokenizer, accelerator)\n",
        "save_results(result_df, RESULT_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg5rsDpa97r7"
      },
      "outputs": [],
      "source": [
        "bias_comparison = ('Basic Estimated Price', 'Anchored Estimated Price')\n",
        "perform_analysis(result_df, BENCHMARK_BIAS_ANALYSIS_FILE_PATH, bias_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1BXgmdB2dYI"
      },
      "source": [
        "# Results - Bias in LLMs\n",
        "\n",
        "## Model Response Analysis\n",
        "- In analyzing the responses from various models, I encountered two types of patterns\n",
        "  1. The model's responses typically led with a valid price, which corresponded accurately to the intended estimations. Thus, I chose to extract the first numerical value generated by the model as the estimated price. This straightforward approach provided a contrast to other models, which required more complex post-processing strategies to identify and extract the most relevant numerical information.\n",
        "  2. Price range representations. I calculated the average of all numerical values mentioned in the response, utilizing the `extract_and_average_validate_price` function. This allowed for a more balanced estimation, factoring in the entire range of numbers provided by the model.\n",
        "- Additionally, as mentioned, during the statistical analysis, any rows associated with prompts that failed to generate numeric estimations were excluded from the analysis.\n",
        "\n",
        "## facebook/opt-350m\n",
        "\n",
        "### Post Processing\n",
        "- first approach\n",
        "\n",
        "### Comparative Results\n",
        "\n",
        "| Comparison                                     | T-Statistic | P-Value                | Mean Abs Difference (Basic) | Mean Abs Difference (Anchored) |\n",
        "|------------------------------------------------|-------------|------------------------|-----------------------------|--------------------------------|\n",
        "| Basic Estimated Price vs. Anchored Estimated Price | 6.91098     | 0.0000000009101749835  | 414.95                      | 179.43                         |\n",
        "\n",
        "\n",
        "## EleutherAI/gpt-j-6B\n",
        "\n",
        "### Post Processing\n",
        "- second approach\n",
        "\n",
        "### Results\n",
        "\n",
        "\n",
        "| Comparison                                     | T-Statistic | P-Value | Mean Abs Difference (Basic) | Mean Abs Difference (Anchored) |\n",
        "|------------------------------------------------|-------------|---------|-----------------------------|--------------------------------|\n",
        "| Basic Estimated Price vs. Anchored Estimated Price | 7.6729      | 0.0000  | 365.14                      | 249.65                         |\n",
        "\n",
        "## mrm8488/mistral-7b-ft-h4-no_robots_instructions\n",
        "\n",
        "### Post Processing\n",
        "- second approach\n",
        "\n",
        "### Results:\n",
        "\n",
        "| Comparison                                     | T-Statistic | P-Value            | Mean Abs Difference (Basic) | Mean Abs Difference (Anchored) |\n",
        "|------------------------------------------------|-------------|--------------------|------------------------------|--------------------------------|\n",
        "| Basic Estimated Price vs. Anchored Estimated Price | 5.13319     | 0.000001747        | 413.75                       | 364.14                         |\n",
        "\n",
        "## Results Discussion\n",
        "\n",
        "The analysis across the three models—facebook/opt-350m, EleutherAI/gpt-j-6B, and mrm8488/mistral-7b-ft-h4-no_robots_instructions—consistently demonstrates the presence of anchoring bias. The statistical results reveal significant differences in how the models respond to basic versus anchored prompts:\n",
        "\n",
        "- **Statistical Significance**: For all three models, the T-statistics are notably positive, and the P-values are well below the threshold of 0.05. This indicates that the models' price estimations are significantly closer to the anchor price when an anchor is present, compared to when it is absent.\n",
        "  \n",
        "- **Impact of Anchoring**: The Mean Absolute Differences further substantiate these findings, with narrower variances in estimations under anchored conditions across all models. This suggests that the models are not just statistically but also practically influenced by the anchoring effect, leading to more constrained estimations that align closely with the anchor prices.\n",
        "\n",
        "These findings underscore the susceptibility of language models to cognitive biases similar to those observed in human decision-making. The consistent pattern of bias highlights the need for robust debiasing mechanisms. Moving forward, enhancing prompt engineering and integrating more sophisticated debiasing strategies will be crucial in developing AI systems that offer unbiased and reliable judgments, especially in critical applications such as dynamic pricing and financial forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debiasing Strategies: Detailed Overview of Prompt Engineering\n",
        "\n",
        "To effectively mitigate the influence of anchoring bias, the debiasing approach centers on sophisticated prompt engineering techniques. These techniques are designed to encourage deeper analytical thinking and reduce reliance on potentially misleading initial price information provided in the prompts.\n",
        "\n",
        "### Types of Prompts Employed\n",
        "Reminder:\n",
        "\n",
        "**1. Basic Prompt:**\n",
        "- **Description**: This prompt serves as a control, setting a neutral baseline for price estimation without introducing any anchoring bias.\n",
        "- **Template**:\n",
        "  ```python\n",
        "  basic_prompt = f\"A fair market price for a {product} is typically.\"\n",
        "  ```\n",
        "- **Purpose**: It allows us to measure the model's unbiased price estimation capabilities and provides a baseline to compare against more complex prompt structures.\n",
        "\n",
        "**2. Anchored Prompt:**\n",
        "- **Description**: Begins with an explicit anchor price to assess its direct influence on the model’s pricing decisions.\n",
        "- **Template**:\n",
        "  ```python\n",
        "  anchored_prompt = f\"In some sale, the price for a high-quality {product} was {anchor_price} dollars. {basic_prompt}\"\n",
        "  ```\n",
        "- **Purpose**: This prompt is crucial for evaluating how strongly an explicit numerical anchor can sway the model’s estimation, serving as a test for the anchoring bias.\n",
        "\n",
        "Current focus:\n",
        "\n",
        "**3. Guided Prompt:**\n",
        "- **Description**: This prompt extends the anchored prompt by integrating contextual cues that guide the model away from anchoring biases and towards more analytical reasoning.\n",
        "- **Template**:\n",
        "  ```python\n",
        "  guided_prompt = f\"In some sale, the price for a high-quality {product} was {anchor_price} dollars. Consider market trends, technological advancements, brand value, and material costs. {basic_prompt}\"\n",
        "  ```\n",
        "- **Purpose**: Designed to enhance the model's price estimation process by incorporating a comprehensive evaluation of relevant factors:\n",
        "  - **Market Trends**: Examines shifts in market dynamics that could impact pricing.\n",
        "  - **Technological Advancements**: Evaluates how recent technological progress could influence costs.\n",
        "  - **Brand Value and Material Costs**: Assesses the brand's reputation and the cost of materials, which are crucial to determining the product's price.\n",
        "\n",
        "This guided prompt is crafted to counteract the initial anchoring effect by broadening the model's understanding and reasoning. It seeks to redirect the model's focus from potentially misleading anchor prices to a holistic analysis, mirroring the decision-making process of well-informed, unbiased consumers.\n",
        "\n",
        "### Enhanced Chain of Thought Technique\n",
        "\n",
        "For the third model, which initially showed signs of bias even after using the guided prompt, I introduced an even more elaborate form of the guided prompt, incorporating the **Chain of Thought** technique with elements of few-shot learning:\n",
        "\n",
        "- **Elaborated Guided Prompt**:\n",
        "  - **Content**: The prompt includes a series of logical reasoning steps that explicitly guide the model through the process of evaluating the product price, ignoring the initial anchor.\n",
        "  - **Example**:\n",
        "```python\n",
        "\"When getting a piece of information, I shouldn't rely too heavily on it and let it be the main factor influencing my judgment. When considering what is a reasonable price for some product, I need to mainly rely on my general knowledge such as market trends, technological advancements, brand value, and material costs. For example, in some store, the price of a smartphone is $200. Considering current market prices and brand value, a fair market price might actually be typically around $800.\"\n",
        "```\n",
        "\n",
        "This version ensures that the entire passage is neatly aligned and presented as a continuous block of text, making it clearer and more readable.\n",
        "\n",
        "- **Purpose**: This expanded prompt aims to model the cognitive process of discounting irrelevant anchors and applying market knowledge, further strengthening the model's ability to independently derive reasoned, unbiased price estimations.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "These debiasing strategies, particularly through refined prompt engineering, form the cornerstone of the efforts to understand and reduce cognitive biases in language models. By crafting prompts that guide the model's thought process, I aimed to reduce the likelihood of decisions skewed by irrelevant numerical anchors."
      ],
      "metadata": {
        "id": "9oCCe-UntDt-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU0Ot9mL2-yG"
      },
      "source": [
        "## Prompt Engineering\n",
        "The guided prompts are intricately designed to integrate substantial contextual information, encouraging the model to consider a broad array of rational factors such as market trends, technological advancements, and material costs. The primary goal is to divert the model's focus away from potentially misleading anchor prices, thereby reducing its biasing effect and fostering more accurate and well-founded price estimations.\n",
        "\n",
        "<img src=\"https://www.techopedia.com/wp-content/uploads/2023/06/Prompt-Engineering-Best-Practices.png\" alt=\"Guide to Prompt Engineering\" width=\"350\">\n",
        "\n",
        "## Few-Shot Learning\n",
        "Few-shot learning — a technique whereby we prompt an LLM with several concrete examples of task performance. This method hopefully not only teaches the model how to approach the task but also enhances its ability to generalize from limited data to new situations effectively.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*4DTL5q--UxuoMFc6P0b3dw.png\" alt=\"Guide to Prompt Engineering\" width=\"400\">\n",
        "\n",
        "## Chain of Thought\n",
        "Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding. The enchanced prompt begin by outlining how to approach the task, focusing on which factors should be considered for reliable price estimation, followed by an example that showcases the reasoning process culminating in a concrete price determination for a given scenario.\n",
        "\n",
        "<img src=\"https://assets-global.website-files.com/640f56f76d313bbe39631bfd/64f22ad73fcc46e507b7d4c7_4rDWGZfr4H2Z4DjtgHRd2hWupSFV9MMkF6zQYWSvAUC-8RSjeghD9ke1np_d2Dip2oloZpMHsB-32czB95Ep8fwBEFoVVK_SvUIcwUFGFvVTGMcMYcIWwW9lK-0rcE2yaZ7ctFit6zbYSpgIt_Krprw.jpeg\" alt=\"Guide to Chain of Thought Prompting\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debiasing adjustments\n",
        "\n",
        "To evaluate the debiasing strategies, I conducted the same test by generating responses to guided prompts, which were designed to include additional context along with the anchored information. This was done to see if the guided prompts—despite containing anchor prices—could lead to estimates that were significantly less influenced by these anchors compared to the direct anchored prompts.\n",
        "\n",
        "### Analysis Procedure\n",
        "The process involved:\n",
        "1. Replacing the neutral basic prompt with the guided prompt that included contextual guidance along with the anchor.\n",
        "2. Comparing the responses to the anchored prompt in terms of their proximity to the anchor price.\n",
        "\n",
        "The statistical assessment, conducted via paired t-tests, revealed:\n",
        "- **Anchored vs. Guided**: A Positive T-Statistic suggested the guided prompts' effectiveness in reducing anchoring bias, evidenced by estimates moving away from the anchor price. The P-Value affirmed these results' statistical significance (<0.05), solidifying the impact of the corrective power of guided information.\n",
        "\n",
        "Below are the scripts used to create the guided prompts:"
      ],
      "metadata": {
        "id": "X18KiAj_unFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spGceaQh3Dxr"
      },
      "outputs": [],
      "source": [
        "# Function to add guided prompts to the existing DataFrame\n",
        "def add_guided_prompt(df, price_column='Anchor Price'):\n",
        "    \"\"\"\n",
        "    Add guided prompts to the existing DataFrame based on the anchor price.\n",
        "    \"\"\"\n",
        "    df['Guided Prompt'] = df.apply(lambda row: f\"In some sale, the price for a high-quality {row['Product']} was {row[price_column]} dollars. \"\n",
        "                                                \"Consider market trends, technological advancements, brand value, and material costs. \"\n",
        "                                                f\"A fair market price for a {row['Product']} is typically \", axis=1)\n",
        "    return df\n",
        "\n",
        "# Load the initial DataFrame from the Excel file\n",
        "df_initial = load_data(DATA_FILE_PATH)\n",
        "\n",
        "# Add the guided prompts\n",
        "df_with_guided = add_guided_prompt(df_initial)\n",
        "\n",
        "# Save the updated DataFrame back to the Excel file\n",
        "df_with_guided.to_excel(DATA_FILE_PATH, index=False)\n",
        "\n",
        "print(f\"Excel file updated with guided prompts at {DATA_FILE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKxJDuq03HYn"
      },
      "outputs": [],
      "source": [
        "def update_guided_prompts_with_few_shot_learning_and_enchanced_chain_of_thought(df):\n",
        "    \"\"\"\n",
        "    Updates the 'Guided Prompt' column of the provided DataFrame with a more explicit chain of thought and an example (few shot learning) to encourage deeper reasoning about price estimations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the chain of thought with an example embedded\n",
        "    chain_of_thought_prompt = (\n",
        "        \"When getting a piece of information, I shouldn't rely too heavily on it, and let it be the main factor influencing my judgement. \"\n",
        "        \"When considering what is a reasonable price for some product, I need to mainly rely on my general knowledge such as market trends, \"\n",
        "        \"technological advancements, brand value, and material costs. For example: In some store, the price of a smartphone is $200. Considering current market prices and brand value, a fair market price might actually be typically around $800. \"\n",
        "    )\n",
        "\n",
        "    # Update the 'Guided Prompt' column by appending the chain of thought before the existing prompt\n",
        "    df['Guided Prompt'] = df.apply(lambda row: chain_of_thought_prompt + row['Anchored Prompt'], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load the DataFrame from the specified path\n",
        "df = load_data(DATA_FILE_PATH)\n",
        "\n",
        "# Update the DataFrame with the chain of thought in the 'Guided Prompt'\n",
        "updated_df = update_guided_prompts_with_few_shot_learning_and_enchanced_chain_of_thought(df)\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "save_results(updated_df, ENHANCED_DATA_FILE_PATH)\n",
        "\n",
        "print(f\"Updated Excel file with chain of thought prompts created at {ENHANCED_DATA_FILE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_debiasing(df):\n",
        "    \"\"\"\n",
        "    Executes debiasing using guided prompts.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): DataFrame containing the guided prompts.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: A new DataFrame containing only the guided responses and estimated prices.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define a helper function to apply to each row\n",
        "    def apply_debiasing(row):\n",
        "        guided_response = generate_text(model, tokenizer, accelerator, row['Guided Prompt'])\n",
        "        guided_price = extract_and_validate_price(guided_response)\n",
        "        return pd.Series([guided_response, guided_price], index=['Guided Response', 'Guided Estimated Price'])\n",
        "\n",
        "    # Create a new DataFrame with the results from applying debiasing to each row\n",
        "    results_df = df.apply(apply_debiasing, axis=1)\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "exftiuFNfiQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sYk_3n8EfQb"
      },
      "outputs": [],
      "source": [
        "# Execute debiasing - using the updated data with the new guided prompt to\n",
        "# get the model response to the guided prompt, and check if it still give bias results\n",
        "\n",
        "few_shot_df = load_data(ENHANCED_DATA_FILE_PATH)\n",
        "few_shot_result_df = execute_debiasing(few_shot_df)\n",
        "\n",
        "result_df = load_data(RESULT_FILE_PATH)\n",
        "result_df['Guided Response'] = few_shot_result_df['Guided Response']\n",
        "result_df['Guided Estimated Price'] = few_shot_result_df['Guided Estimated Price']\n",
        "\n",
        "# update the results file with the debias results as well\n",
        "save_results(result_df, RESULT_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = load_data(RESULT_FILE_PATH)\n",
        "debias_comparison = ('Guided Estimated Price', 'Anchored Estimated Price')\n",
        "perform_analysis(result_df, DEBIASING_ANALYSIS_FILE_PATH, debias_comparison)"
      ],
      "metadata": {
        "id": "cxWdz-U-U3BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItbfSI2BZSbe"
      },
      "source": [
        "# Debiasing Results\n",
        "\n",
        "# facebook/opt-350m\n",
        "\n",
        "## Post Processing\n",
        "- first approach\n",
        "\n",
        "## Comparative Results\n",
        "\n",
        "| Comparison                                     | T-Statistic | P-Value            | Mean Abs Difference (Guided) | Mean Abs Difference (Anchored) |\n",
        "|------------------------------------------------|-------------|--------------------|------------------------------|--------------------------------|\n",
        "| Guided Estimated Price vs. Anchored Estimated Price | 3.76922     | 0.0002651722399    | 231.83                       | 158.77                         |\n",
        "\n",
        "# EleutherAI/gpt-j-6B\n",
        "\n",
        "## Post Processing\n",
        "- second approach\n",
        "\n",
        "## Results\n",
        "\n",
        "| Comparison                                 | T-Statistic | P-Value          | Mean Abs Difference (Guided) | Mean Abs Difference (Anchored) |\n",
        "|--------------------------------------------|-------------|------------------|------------------------------|--------------------------------|\n",
        "| Guided Estimated Price vs. Anchored Price  | 4.1987      | 0.000053         | 286.01                       | 239.67                         |\n",
        "\n",
        "\n",
        "# mrm8488/mistral-7b-ft-h4-no_robots_instructions\n",
        "\n",
        "## Post Processing\n",
        "- second approach\n",
        "\n",
        "## Results:\n",
        "\n",
        "### Prompt Engenireeing\n",
        "\n",
        "| Comparison                                       | T-Statistic       | P-Value          | Mean Abs Difference (Guided) | Mean Abs Difference (Anchored) |\n",
        "|--------------------------------------------------|-------------------|------------------|------------------------------|--------------------------------|\n",
        "| Guided Estimated Price vs. Anchored Estimated Price | -0.05413          | 0.95692          | 363.27                       | 363.63                         |\n",
        "\n",
        "The analysis shows that while there is a statistically significant anchoring effect when comparing basic vs. anchored prompts, the attempt to debias through guided prompts did not result in a statistically significant improvement (P-Value > 0.05). However, some improvement in the mean differences was observed.\n",
        "\n",
        "### Chain of Thought enhancement\n",
        "\n",
        "| Comparison                                      | T-Statistic | P-Value | Mean Abs Difference (Guided) | Mean Abs Difference (Anchored) |\n",
        "|-------------------------------------------------|-------------|---------|------------------------------|--------------------------------|\n",
        "| Guided Estimated Price vs. Anchored Estimated Price | 7.43967     | 0.00000 | 428.82                       | 364.88                         |\n",
        "\n",
        "This approach significantly debiased the model's responses, with the guided prompt's results showing a considerable reduction in the mean difference from the anchor (P-Value < 0.05). This indicates a successful debiasing intervention.\n",
        "\n",
        "## Results Discussion\n",
        "\n",
        "The debiasing analysis across all three models—facebook/opt-350m, EleutherAI/gpt-j-6B, and mrm8488/mistral-7b-ft-h4-no_robots_instructions—demonstrates varying degrees of response to the guided prompts. Notably, each model showed distinct outcomes, which provide valuable insights into the effectiveness of our debiasing strategies:\n",
        "\n",
        "- **Statistically Significant Influence**: For all models, the positive T-statistics paired with P-values below 0.05 indicate that the guided prompts were effective in influencing the model's pricing responses away from the anchor price. This suggests that the additional contextual information provided in the guided prompts helped moderate the anchoring bias.\n",
        "  \n",
        "- **Mean Absolute Differences**: While there were improvements in the mean absolute differences between the guided and anchored price estimations, the extent of change varied across models. This variance highlights the nuanced impact of contextual factors included in the guided prompts on different model architectures and their inherent processing capabilities.\n",
        "\n",
        "### Enhanced Chain of Thought\n",
        "\n",
        "The \"Chain of Thought\" enhancement, particularly for the mrm8488/mistral-7b-ft-h4-no_robots_instructions model, further substantiated the potential of sophisticated prompting techniques in debiasing. The guided prompts that incorporated a sequential reasoning approach significantly reduced the mean difference from the anchor price, with P-values underscoring the statistical significance of these results.\n",
        "\n",
        "This underscores a successful debiasing intervention, affirming that the strategic inclusion of detailed reasoning and contextual analysis in prompts can effectively counteract anchoring biases. Such enhancements not only guide models toward more balanced evaluations but also foster a deeper understanding of contextual influences, thereby improving the overall reliability of the model's judgments.\n",
        "\n",
        "### Concluding Insights\n",
        "\n",
        "These results exemplify the potential of carefully designed guided prompts, enriched with context and reasoning pathways, to mitigate biases in language models. Moving forward, the insights gained from these experiments will inform the development of more robust models capable of delivering unbiased and accurate responses across a variety of scenarios. This study lays a foundation for future research aimed at refining these techniques and exploring new strategies to enhance the decision-making capabilities of AI systems in real-world applications.\n",
        "\n",
        "\n",
        "<img src=\"https://media.licdn.com/dms/image/D4E12AQFo4fVHRb7BSA/article-cover_image-shrink_720_1280/0/1695822828192?e=2147483647&v=beta&t=90kbf-0AJfWXZOkeEj8MzOvvdcXKkqsv-LBd1lWu6RI\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ld-nnbtywn8y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}